{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Machine Learning:**  \n",
    "Building computational artifacts that learn over time based on experience. (Building, Science, Math, Engineering, Computation)\n",
    "\n",
    "# Classification vs Regression\n",
    "**Supervised Learning** - Take examples of inputs and outputs. Now, given a new input, predict its putput\n",
    "* Classification <- covered in this lesson\n",
    "* Regression\n",
    "\n",
    "**Classification:**  \n",
    "Taking some input x and mapping it to a descrete number of labels.\n",
    "\n",
    "**Regression:**\n",
    "Continuous valued functions.  Example is a regression line.\n",
    "\n",
    "# Classification Learning\n",
    "* **Instances (Input)** - Vectors of atributes that define the input space (ex pictures, credit scores)\n",
    "* **Concept (Functions)** - Maps inputs to outputs.  Mapping between objects in the world and membership in a set.\n",
    "* **Target Concept (Answer)** - Thing we are trying to find\n",
    "* **Hypothesis [Class]** - Set of all concepts that you're willing to think about\n",
    "* **Sample (Training Set)** - Set of all inputs paired with a label that is the correct output.\n",
    "* **Candidate** - A concept that I think may find the target concept. \n",
    "* **Testing Set** - Looks like the training set. Determin whether candidate does a good job or not.\n",
    "\n",
    "# Regression\n",
    "**Regress to the mean**\n",
    "![Regression](Lesson5/Regression-and-Function-Approx.png)\n",
    "\n",
    "Reinforcement Learning <- Word's meaning used incorrectly\n",
    "\n",
    "## Linear Regression\n",
    "### Regression in Machine Learning\n",
    "![Regression](Lesson5/Regression-Housing-1.png)\n",
    "\n",
    "**How do we find the Best Fit Line? Use calculus!!**\n",
    "![Regression](Lesson5/Finding-Best-Constant-Function.png)\n",
    "\n",
    "### Order of Polynomial\n",
    "k = 0: constant  \n",
    "k = 1: line  \n",
    "k = 2: parabola\n",
    "\n",
    "$f(x)=c_0 + c_1x + c_2x^2 + \\cdots + c_kx^k$\n",
    "\n",
    "![Regression Matrices](Lesson5/Poly-Regression-Matrices.png)\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "Xw &\\approx& y \\\\\n",
    "X^TXw &\\approx& X^Ty \\\\\n",
    "(X^TX)^{-1}X^TXw &\\approx& (X^TX)^{-1}X^Ty \\\\\n",
    "w &\\approx& (X^TX)^{-1}X^Ty \\\\\n",
    "\\end{eqnarray}$\n",
    "\n",
    "### Errors\n",
    "Training data has errors not modeling $f$, but $f+\\epsilon$  \n",
    "**Where do errors come from?**\n",
    "* sensor error\n",
    "* maliciously - being given bad data\n",
    "* transcription error\n",
    "* unmodeled influences\n",
    "\n",
    "### Cross Validation\n",
    "**The Goal is to be able to generalize!!!!**\n",
    "\n",
    "# Logistic Regression\n",
    "![Logistic Matrices](Lesson5/Logistic-Regression.png)\n",
    "\n",
    "Plot an initial line on the graph.  See how many points are missclassified.  Adjust the line to see if an improvement can be made.\n",
    "\n",
    "**J/K! # of errors is not what has to be minimized!**\n",
    "\n",
    "## Log Loss Function\n",
    "* Assign larger penalty to wrongly classified points\n",
    "* Assign small penalty to correctly classified points.\n",
    "* Sum the errors\n",
    "* **Now joggle the line around to minimize the error**\n",
    "\n",
    "** Find the best line fit by minimzzing the error function**\n",
    "\n",
    "# Neural Networks\n",
    "* What if a line is to simple to classify the data\n",
    "\n",
    "We will use two lines for now.  Will use a similar approach as before, using gradient decent.\n",
    "![Neural Networks](Lesson5/Neural-Networks.png)\n",
    "\n",
    "![NN-Simple-Layers](Lesson5/NN-Simple-Layers.png)\n",
    "\n",
    "# Perceptron\n",
    "**Perceptron** - Individual node in a neural network that can be connected to other nodes..  It is the basic unit of the NN.Each one looks at input data and decides how to categorize that data.\n",
    "\n",
    "## Weights\n",
    "How does the NN know which input has more importance when determining the output decision?  It uses **weights**.  When input data comes into a perception, it gets multiplied by a weight value that is assigned to this particular input.  Weights start off as random values, as the neural networks learn more about the input data and the output it produces, it adjusts the weights to minimize the errors in categorization that the previous weights resulted in.  This is **training** the NN.\n",
    "\n",
    "The higher the weight ==> the more important the input is.\n",
    "\n",
    "## Summing the Input Data\n",
    "![Summing the Input Data](Lesson5/perceptron-graphics-001.jpeg)\n",
    "\n",
    "Weighted input is summed up to produce a single output value.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^m w_i \\cdot x_i\n",
    "\\end{equation*}\n",
    "\n",
    "In NN equations, weights will be represented using w.  It will be $W$ for a matrix of weights and $w$ for an individual weight and it may include a subscript.\n",
    "\n",
    "## Calculating the Output with an Activation Function\n",
    "Feed the linear combination into an activation function.  This is at the node level, not the whole NN level.  The output of a layer is usually called its activations.  Activation functions determine what should be the node's output, given the input.  \n",
    "\n",
    "One of the simplist ativation functions: **Heaviside Step Function:**\n",
    "\\begin{equation*}\n",
    "f(h) =\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\mbox{if $h < 0$} \\\\\n",
    "            1 & \\mbox{if $h \\geq 0$}\n",
    "        \\end{array}\n",
    "      \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "If using weights $w_{grades} = -1$ and $w_{test} = -0.2$ the output range is $(-\\infty,0]$.  SO the activation funciton can only produce 1 when both tests and grades are 0.\n",
    "\n",
    "![activation function](Lesson5/Activation-Function-1.png)\n",
    "\n",
    "This is no bueno.  We want more combinations of tests and grades to be accepted.  So to solve this we add a bias, $b$ to the linear combination.  The resulting graph is.\n",
    "\n",
    "![Example after bias](Lesson5/example-after-bias.png)\n",
    "\n",
    "Just like the weights, the bias is assigned a random value and the NN learns and adjust the bias to minimize missclassifications.  So the complete perceptron formula is:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x_1, x_2, \\ldots, x_m) = \n",
    "    \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\mbox{if $b + \\Sigma w_i \\cdot x_i < 0$} \\\\\n",
    "            1 & \\mbox{if $b + \\Sigma w_i \\cdot x_i \\geq 0$}\n",
    "        \\end{array}\n",
    "     \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "**The power of a neural network isn't building it by hand, like we were doing. It's the ability to learn from examples.**\n",
    "\n",
    "## The Simplest Neural Network\n",
    "A diagram of a simple network\n",
    "![Diagram of Simple Network](Lesson5/simple-neuron.png)\n",
    "\n",
    "So $f(h)$ can be anything.  The one we will see the most is the logistic (sigmoid) function.\n",
    "\\begin{equation*}\n",
    "    \\text{sigmoid($x$)} = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "![sigmoid](Lesson5/sigmoid.png)\n",
    "\n",
    "* Bound to 0 and 1\n",
    "* As an output, it can be interpreted as a probability for success.\n",
    "* Using a sigmoid as the activation function results in the same formulation as logistic regression.\n",
    "\n",
    "## Gradient Descent\n",
    "### Learning Weights\n",
    "\n",
    "**Need to learn weights from example data, then use those weights to make predictions.**\n",
    "\n",
    "We will use the **error** to guage how correct the weights are.  The type of error we use is the sum of the squared errors (SSE)\n",
    "\\begin{equation*}\n",
    "    E = \\frac{1}{2} \\Sigma_\\mu \\Sigma_j [y_j^\\mu - \\hat y_j^\\mu]^2\n",
    "\\end{equation*}\n",
    "* $\\hat y$: the prediction\n",
    "* $y$: the true value\n",
    "* $j$: all output units\n",
    "* $\\mu$: all data points\n",
    "\n",
    "Bebefits of SSE:\n",
    "* Error is always (+)\n",
    "* Larger errors are penalized more than smaller errors\n",
    "\n",
    "The prediction depends on the weights\n",
    "\\begin{equation*}\n",
    "    \\hat y_j^\\mu = f(\\Sigma_i w_{ij} x_i^\\mu)\n",
    "\\end{equation*}\n",
    "\n",
    "So therefore the error depends on the weight:\n",
    "\\begin{equation*}\n",
    "    E = \\frac{1}{2} \\Sigma_\\mu \\Sigma_j [y_j^\\mu - f(\\Sigma_i w_{ij} x_i^\\mu)]^2\n",
    "\\end{equation*}\n",
    "\n",
    "**Goal:** find weights $w_{ij}$ that minimize the squared error E.  Use gradient descent to accomplish this.  We want to change the weights in the \"direction\" that minimizes the error the most.  Find direction by calculating the *graident*.  *Gradient* is another term for rate of change/slope.\n",
    "\n",
    "So now we will be using derivitives!  A derivative of a function $f(x)$ gives another function $f'(x)$ that returns the slope of $f(x)$ at point $x$.\n",
    "\n",
    "![Derivative Example](Lesson5/derivative-example.png)\n",
    "\n",
    "The gradient is just a derivative generalized to functions with more than one variable. We can use calculus to find the gradient at any point in our error function, which depends on the input weights. \n",
    "\n",
    "### Caveats\n",
    "**Local minima**\n",
    "\n",
    "### Gradients from Khan Academy (Supplemental Material)\n",
    "![Gradient 1](Lesson5/Khan-Gradient-1.png)\n",
    "\n",
    "![Gradient 2](Lesson5/Khan-Gradient-2.png)\n",
    "\n",
    "![Gradient 3](Lesson5/Khan-Gradient-3.png)\n",
    "\n",
    "![Gradient 4](Lesson5/Khan-Gradient-4.png)\n",
    "\n",
    "![Gradient 5](Lesson5/Khan-Gradient-5.png)\n",
    "\n",
    "![Gradient 6](Lesson5/Khan-Gradient-6.png)\n",
    "\n",
    "### Gradient Descent: The Math\n",
    "**Simple NN**\n",
    "![Gradient Math 1](Lesson5/Gradient-Math-1.png)\n",
    "\n",
    "Like to use output to make predictions?  How do we build the network when we don't know the weights?  We give it data that we know is true then set the weights to match data.  **Need some measure to measure how bad our predictions are.  Use the Sum of the Squared Errors (SSE)**\n",
    "\n",
    "\\begin{align}\n",
    "    E &= \\frac{1}{2} \\sum_{\\mu} (y^\\mu - \\hat y^\\mu)^2 \\\\\n",
    "      &= \\frac{1}{2} \\sum_{\\mu} (y^\\mu - f(\\sum_i w_i x_i^\\mu))^2\n",
    "\\end{align}\n",
    "\n",
    "![Gradient Math 2](Lesson5/Gradient-Math-2.png)\n",
    "\n",
    "![Gradient Math 3](Lesson5/Gradient-Math-3.png)\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial E}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\frac{1}{2} (y - \\hat y)^2 \\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i} \\frac{1}{2} (y - \\hat y(w_i))^2\n",
    "\\end{align}\n",
    "\n",
    "![Gradient Math 4](Lesson5/Gradient-Math-4.png)\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial E}{\\partial w_i}\\ &=\\ \\frac{\\partial}{\\partial w_i} \\frac{1}{2} (y - \\hat y)^2  &:& \\text{ using chain rule} \\\\\n",
    "    &=\\ (y-\\hat y)\\frac{\\partial}{\\partial w_i}(y - \\hat y) &:& \\text{ Remove target val because doesn't depend on weights. Use chain rule again} \\\\\n",
    "    &=\\ -(y-\\hat y)\\frac{\\partial \\hat y}{\\partial w_i} &:&\\ \\hat y = f(h) \\text{ where } h=\\sum_i w_ix_i \\\\\n",
    "    &=\\ -(y-\\hat y)f'(h)\\frac{\\partial}{\\partial w_i}\\sum w_ix_i &:&\\ \\frac{\\partial}{\\partial w_i}\\sum w_ix_i\n",
    "           = \\frac{\\partial}{\\partial w_1}[w_1x_1+...+w_nx_n] = x_1 + 0 + ...\\ =\\ x_i   \\\\\n",
    "    &=\\ -(y-\\hat y)f'(h)x_i &:&\n",
    "            \\text{ The negative of the error times the derivitive of the activation functino at h times the input val } x_i \\\\\n",
    "    \\Delta w_i\\ &=\\ \\eta (y-\\hat y)f'(h)x_i\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Error term**\n",
    "\\begin{equation*}\n",
    "    \\delta = (y-\\hat y)f'(h) \\\\\n",
    "    w_i = w_i + \\eta \\delta x_i\n",
    "\\end{equation*}\n",
    "\n",
    "Working with multiple output units.  Think of it as stacking things.\n",
    "\n",
    "![Gradient Math 5](Lesson5/Gradient-Math-5.png)\n",
    "\n",
    "![Gradient Math 6](Lesson5/Gradient-Math-6.png)\n",
    "\n",
    "### Gradient Descent: The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(x[0]*w[0] + x[1]*w[1])\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error * sigmoid_prime(np.dot(x,w)) * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient Descent\n",
    "\n",
    "Use gradient descent to train a network on graduate school admissions data.\n",
    "\n",
    "**Inputs**  \n",
    "* &nbsp;&nbsp;&nbsp;&nbsp;GRE Score\n",
    "* &nbsp;&nbsp;&nbsp;&nbsp;GPA\n",
    "* &nbsp;&nbsp;&nbsp;&nbsp;Rank of undergrad School\n",
    "\n",
    "**Output**\n",
    "* &nbsp;&nbsp;&nbsp;&nbsp;Admitted/Rejected\n",
    "\n",
    "#### Data cleanup\n",
    "\n",
    "Data needs to be transformed before we can use it.  Rank feature is categorical, so we need to use dummy variables to encode it.  Split data into four new columns encoded with ones and zeros.\n",
    "\n",
    "Stanardize GRE and GPA data.  Scale so they have a 0 mean and a standard deviation of 1.  **Necessary because the sigmoid function aquashes really small and really large inputs.**  \n",
    "\n",
    "![Example data](Lesson5/example-data.png)\n",
    "\n",
    "#### Mean Square Error\n",
    "Use Mean of Square Error instead of SSE.  Because if we use the SSE and we sum up a lot of data, it will lead to large updates and make the gradient descent diverge.  Formula is:\n",
    "\\begin{equation*}\n",
    "    E = \\frac{1}{2m}\\sum_\\mu(y^\\mu - \\hat y^\\mu)^2\n",
    "\\end{equation*}\n",
    "\n",
    "**Algorithm for updating the weights with gradient descent**\n",
    "* Set the weight step to zero: $\\Delta w_i = 0$\n",
    "* For each record in the training data:\n",
    "    * Make a forward pass through the network, calculating the output $\\hat y = f(\\sum_i w_i x_i)$\n",
    "    * Calculate the error gradient in the output unit, $\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)$\n",
    "    * Update the weight step $\\Delta w_i = \\Delta w_i + \\delta x_i$\n",
    "* Update the weights $w_i = w_i + \\eta\\Delta w_i/m$ where $\\eta$ is the learning rate and $m$ is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "* Report for $e$ epochs.\n",
    "\n",
    "You can also update the weights on each record instead of averaging the weight steps after going through all the records.\n",
    "\n",
    "Activation function we are using is the sigmoid.  The function and it's derivative are:\n",
    "\\begin{equation*}\n",
    "    f(h) = \\frac{1}{1+e^{-h}} \\\\\n",
    "    f'(h)= f(h)(1-f(h))\n",
    "\\end{equation*}\n",
    "\n",
    "#### Implementing with Numpy\n",
    "Firts, initialize weights with small values.  Important to initialize them randomly.  Good value for scale is $1/\\sqrt{n}$, where n is the number of input units.\n",
    "```python\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "```\n",
    "\n",
    "Linear combination\n",
    "```python\n",
    "# input to the output layer\n",
    "output_in = np.dot(weights, inputs)\n",
    "```\n",
    "\n",
    "##### data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('Lesson5/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = (y - output)\n",
    "\n",
    "        # TODO: Calculate change in weights\n",
    "        del_w += error * output * (1 - output) * x\n",
    "\n",
    "        # TODO: Update weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "![Multilater perceptrons 1](Lesson5/Multilayer-Perceptrons-1.png)\n",
    "\n",
    "## Implementing the hidden layer\n",
    "### Derivation\n",
    "We now require two indices on the weights: $w_{ij}$. Where $i$ denotes input units and $j$ are hidden units.\n",
    "![NN with hidden layers](Lesson5/NN-with-hidden-layers.png)\n",
    "\n",
    "![NN with weights](Lesson5/Hidden-with-weights.png)\n",
    "\n",
    "![NN with weights and matrix](Lesson5/NN-weights-matrix.png)\n",
    "\n",
    "To initialize the weights in python\n",
    "```python\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "For each hidden later, $h_j$, we need to calculate the following:\n",
    "\\begin{equation*}\n",
    "    h_j = \\sum_i w_{ij} x_i\n",
    "\\end{equation*}\n",
    "\n",
    "![input weight multiplication](Lesson5/NN-input-weight-mult.png)\n",
    "\n",
    "$$\n",
    "h_1 = x_1w_{11}+x_2w_{21}+x_3w_{31}\n",
    "$$\n",
    "\n",
    "In numpy you can use:\n",
    "```python\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "```\n",
    "### Making a Column Vector\n",
    "```python\n",
    "print(features)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.T)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features[:, None])\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "       \n",
    "###     Or       ###\n",
    "np.array(features, ndmin=2)\n",
    "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
    "\n",
    "np.array(features, ndmin=2).T\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```\n",
    "#### Programming quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "![Backpropagation Diagram](Lesson5/backpropagation-1.png)\n",
    "\n",
    "Backpropagation is the extension of updating weights using gradient descent.  It uses the chain rule to find the error with the respect to the weights connecting the input later to the hidden later (for a two later network).\n",
    "\n",
    "A weights determines how much of an effect the input has on the output at a certain node.  Since we know the weights in the output node, we can work our way backwards.  For example, in the output layer, you have errors $\\delta_k^o$ attributed to each output unit $k$.  Then, the error attributed to hidden unit $j$ is the output errors, scaled by the weights between the output and hidden layers (and the gradient)\n",
    "$$\n",
    "\\delta_j^h = \\sum W_{jk}\\delta_k^of'(h_j)\n",
    "$$\n",
    "\n",
    "Gradient descent step is the same as before, just with new errors:\n",
    "$$\\Delta w_{ij} = \\eta\\delta_j^h x_i $$\n",
    "\\begin{align}\n",
    "w_{ij} :&\\text{ weights between the inputs and hidden later} \\\\\n",
    "x_i :&\\text{ input unit values}\n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\\Delta w_{pq} = \\eta\\delta_{output}V_{in}\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\delta_{output} :& \\text{ output error} \\\\\n",
    "    V_{in} :& \\text{ input to the layer, the hidden layer activations to the output for example}\n",
    "\\end{align}\n",
    "\n",
    "## Working through an example\n",
    "![backprop-network.png](Lesson5/backprop-network.png)\n",
    "\n",
    "* Start with a forward pass\n",
    "    * Calculate the input to the hidden unit\n",
    "    $$h = \\sum_iw_ix_i = 0.1 \\times 0.4 - 0.1 \\times 0.3 = -0.02$$\n",
    "    * Calulate the output of the hidden unit\n",
    "    $$a = f(h) = \\text{sigmoid}(-0.02) = 0.495$$\n",
    "    * Using this as the input to the output unit, the output is\n",
    "    $$\\hat y = f(W \\cdot a) = \\text{sigmoid}(0.1 \\times 0.495) = 0.512$$\n",
    "* Start the backwards pass to calculate the weight updates for both layers.\n",
    "    * Error of the output unit is\n",
    "    $$\\delta^o=(y - \\hat y)f'(W \\cdot a) = (1 - 0.512) \\times 0.512 \\times (1 - 0.512) = 0.122$$\n",
    "    * Calculate error for the hidden unit.  For the hidden unit error, $\\delta_j^h=\\Sigma_kW_{jk}\\delta_k^of'(h_j)$\n",
    "    $$\\delta^h = W\\delta^of'(h) = 0.1 \\times 0.122 \\times 0.495 \\times (1 - 0.495) = 0.003$$\n",
    "* Now that we have errors, we can calculate the gradient descent steps\n",
    "    * Hidden to output weight step\n",
    "    $$\\Delta W = \\eta\\delta^oa = 0.5 \\times 0.122 \\times 0.495 = 0.0302$$\n",
    "    * Input to hidden weights $w_i$\n",
    "    $$\\Delta w_i = \\eta\\delta^hx_i = (0.5 \\times 0.003 \\times 0.1,\\ 0.5 \\times 0.003 \\times 0.3) = (0.00015,\\ 0.00045)$$\n",
    "    \n",
    "From this example, you can see one of the effects of using the sigmoid function for the activations. The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the vanishing gradient problem. Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures.\n",
    "\n",
    "## Implementing in Numpy\n",
    "However, previously we were only dealing with error terms from one unit. Now, in the weight update, we have to consider the error for each unit in the hidden layer, $\\delta_j$\n",
    "$$\\Delta w_{ij} = \\eta\\delta_jx_i$$\n",
    "Firstly, there will likely be a different number of input and hidden units, so trying to multiply the errors and the inputs as row vectors will throw an error\n",
    "\n",
    "Also, $w_{ij}$ is a matrix now, so the right side of the assignment must have the same shape as the left side. Luckily, Numpy takes care of this for us. If you multiply a row vector array with a column vector array, it will multiply the first element in the column by each element in the row vector and set that as the first row in a new 2D array. This continues for each element in the column vector, so you get a 2D array that has shape (len(column_vector), len(row_vector)).\n",
    "\n",
    "## Backpropagation exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "del_err_output = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = (weights_hidden_output * del_err_output\n",
    "                  * hidden_layer_output * (1 - hidden_layer_output))\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * del_err_hidden * x[:,None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing backpropagation\n",
    "**Error in output layer**\n",
    "$$\\delta_k = (y_k - \\hat y_k)f'(a_k)$$\n",
    "**Error in the hidden layer**\n",
    "$$\\delta_j = \\sum[w_{jk}\\delta_k]f'(h_j)$$\n",
    "\n",
    "### General Algorithm\n",
    "* Set the weight steps for each layer to zero\n",
    "    * The input to hidden weights $\\Delta w_{ij} = 0$\n",
    "    * The hidden to output weights $\\Delta W_j = 0$\n",
    "* For each record in the training data:\n",
    "    * Make a forward pass through the network, calculating the output $\\hat y$\n",
    "    * Calculate the error gradient in the output unit, $\\delta^o = (y - \\hat y)f'(z) \\text{ where } z = \\sum_j W_ja_j$ the input to the output unit\n",
    "    * Propagate the errors to the hedden layer $\\delta_j^h = \\delta^oW_jf'(h_j)$\n",
    "    * Update the weight steps:\n",
    "        * $\\Delta W_j = \\Delta W_j + \\delta^oa_j$\n",
    "        * $\\Delta w_{ij} = \\Delta w_{ij} + \\delta_j^ha_i$\n",
    "* Update the weights, when $\\eta$ is the learning rate and $m$ is the number of records:\n",
    "    * $W_j = W_j + \\eta\\Delta W_j/m$\n",
    "    * $w_{ij} = w_{ij} + \\eta\\Delta w_{ij}/m$\n",
    "* Repeat for $e$ epochs\n",
    "    \n",
    "## Backpropagation exercise\n",
    "### data_prep_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('Lesson5/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(21)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error gradient in output unit\n",
    "        output_error = error * output * (1 - output)\n",
    "\n",
    "        # TODO: propagate errors to hidden layer\n",
    "        hidden_error = weights_hidden_output * output_error \\\n",
    "                       * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error * hidden_output\n",
    "        del_w_input_hidden += hidden_error * x[:,None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden/n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output/n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
