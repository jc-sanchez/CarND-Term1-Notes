{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hello, Tensor World!\n",
    "## Tensor\n",
    "\n",
    "Data is encapsulated in an object called a tensor. `hello_constant = tf.constant('Hello World!')`, hello_constant is a 0-dimensional string tensor.  But there are more.\n",
    "```python\n",
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    " # C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])\n",
    "```\n",
    "\n",
    "Tensor returned by `tf.constant` is called a constant tensor, because the value of the tensor never changes.\n",
    "\n",
    "## Session\n",
    "API is built around the idea of a computational graph, a way of visualizing a mathematical process.  TensorFlow code as graph:\n",
    "![Session](Lesson6/Session.png)\n",
    "\n",
    "\"TensorFlow Session\" is an environment for running a graph.  Session is in charge of allocating the operations to GPUs/CPUs, including remote machines.\n",
    "\n",
    "# TensorFlow Input\n",
    "Go over the basics of feeding data into TensorFlow.\n",
    "\n",
    "## tf.placeholder()\n",
    "Can't set x to data set and put it in TensorFlow.  **tf.placeholder()** returns a tensor that gets its value from data passed to the **tf.session.run()**\n",
    "\n",
    "## Session's feed_dict\n",
    "```python\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: \"Hello World\"})\n",
    "```\n",
    "\n",
    "Use the **feed_dict** parameter in **tf.session.run()** to set the placeholder tensor. Possible to set more than one tensor:\n",
    "```python\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "```\n",
    "\n",
    "# TensorFlow Math\n",
    "## Addition\n",
    "```python\n",
    "x = tf.add(5, 2)\n",
    "```\n",
    "\n",
    "It takes in two numbers, two tensors, or one of each, and returns their sum as a tensor.\n",
    "\n",
    "## Subtraction and Multiplication\n",
    "```python\n",
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10\n",
    "```\n",
    "\n",
    "## Converting Types\n",
    "It may be necessary to convert between types to make certain operators work together, i.e. floats and ints.\n",
    "\n",
    "You can make sure your data is all of the same type, or you can cast a value to another type.\n",
    "```python\n",
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))\n",
    "```\n",
    "\n",
    "## Quiz\n",
    "```python\n",
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x, y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)\n",
    "```\n",
    "\n",
    "# Supervised Classification\n",
    "* Take an input and get an output, e.i. identify a letter.\n",
    "* Use training set with already labeled data\n",
    "* We will be learing about a logistic classifier\n",
    "\n",
    "# Training Your Logistic Classifier\n",
    "* It is linear\n",
    "* Takes input and applies a linear function to generate its prediction\n",
    "* Pretty much a matrix multiplier\n",
    "* A lot like last lesson\n",
    "* turn scores into probability of correct label.\n",
    "    * Use soft max function to turn scores into probablilites\n",
    "$$\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_i}}\n",
    "$$\n",
    "\n",
    "* Scores of logistic regression are also called logits\n",
    "\n",
    "# TensorFlow Linear Function\n",
    "Let's derive function **y = Wx + b**\n",
    "\n",
    "First it's **y = Wx**\n",
    "![wx 1](Lesson6/wx-1.jpg)\n",
    "\n",
    "**y = Wx + b**\n",
    "![wx 1](Lesson6/wx-b.jpg)\n",
    "\n",
    "## Transposition\n",
    "Instead of **y = Wx + b**, we will be using **y = xW + b**.  They are very similar, except that they have been transposed and the rows and the columns have been switched.  This is what TensorFlow uses.\n",
    "\n",
    "## Weights and Bias in TensorFlow\n",
    "Goal of training a NN is to modify the weights ans biases to best predict the labels.  A tensor that can be modified must be used.  Rules out **tf.placeholder()** and **tf.constant()**. We will use **tf.variable**\n",
    "\n",
    "### tf.variable()\n",
    "```python\n",
    "x = tf.Variable(5)\n",
    "```\n",
    "\n",
    "Tensor stores its state in the session, the state of the tensor must be initialized manually.  Use **tf.global_variables_initialize()**\n",
    "\n",
    "```python\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "```\n",
    "\n",
    "**tf.global_variables_initialize()** call returns an operation that will initialize all TensorFlow variables from the graph.  Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it.\n",
    "\n",
    "Choosing weights from a normal distribution prevents any one weight from overwhilming other weights.  Use **tf.truncate_normal()** to generate rendom numbers from a normal distribution.\n",
    "\n",
    "### tf.truncated_normal()\n",
    "```python\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "```\n",
    "\n",
    "The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.  Set bias to 0.\n",
    "\n",
    "### tf.zeros()\n",
    "```python\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "```\n",
    "\n",
    "## Quize\n",
    "### quiz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"quiz_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### sandbox.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"sandbox_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Softmax\n",
    "Next step is to assign a probability to each label, which you can then use to classify the data.  Here is the formula again.\n",
    "$$\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_i}}\n",
    "$$\n",
    "\n",
    "By taking \"e\" to the power of any real value we always get back a positive value.\n",
    "```python\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "```\n",
    "\n",
    "# One-Hot Encoding\n",
    "![NN Confidence](Lesson6/nn-confidence.png)\n",
    "\n",
    "If size of outputs is increased, claassifier becomes bery confident about its predictions.  If you reduce it is unsure.  You want to start the classifier to not be sure in the beginning then gain confidence as it learns.  Represent labels by using a vector that has a 1 in the location of it's corresponding label. \n",
    "\n",
    "Works in many problems, but it does not scale very well.  It is made up of vectors that contain mostly zeros.  Will deal with the problem using embeddings later on.\n",
    "\n",
    "# Cross Entropy\n",
    "Nice thing about approach is that it allows us to see how well we are doing by comparing two vectors.\n",
    "![cross entropy](Lesson6/cross-entropy.png)\n",
    "\n",
    "## Multinomial Logistic Classification\n",
    "$$\n",
    "D(S(WX + b), L)\n",
    "$$\n",
    "![mls](Lesson6/mlc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(one_hot * tf.log(softmax))\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict={softmax:softmax_data, one_hot:one_hot_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimize Cross Entropy\n",
    "How are we going to find those weights, $w$, and biases $b$, that will get our classigier to do what we want it to do.  Have a low distance for the correct class and high distance for the incorrect class.  Measure distance averaged over the entire training set.  That is the training loss.  Try to minimize loss by using gradient descent.\n",
    "![Loss](Lesson6/loss.png)\n",
    " \n",
    "# Normalized Inputs and Initial Weights\n",
    "We want 0 mean and equal variance.\n",
    "**Mean:** $X_i = 0$  \n",
    " \n",
    "**Variance:** $\\sigma(X_i) = \\sigma(X_j)$\n",
    "![Normalized](Lesson6/normalized.png)\n",
    " \n",
    "![Normalized Pics](Lesson6/normalized-pics.png)\n",
    " \n",
    "Large sigma means that dist will have large peaks, very opinionated.\n",
    "Small sigma, very uncertain abolut things.  Better to begin with uncertain distribution.\n",
    " \n",
    "![Init](Lesson6/init.png)\n",
    "  \n",
    "# Measuring Performance\n",
    "This section is about the classifier not being able to gerneralize when released into production.  It was able to classify the testing data correctly but failed when given new data.  This is because when you train and test the classifier, you are bleeding some of the information from the test set into the training set.  One solution is to split the data into 3 piles, train, validate, and test.  You never look at the test set.\n",
    "\n",
    "# Validation and Test Set Size\n",
    "\n",
    "The bigger the test set the less noise you have in the accuracy measurement.\n",
    "\n",
    "**Rule of thumb** Change that affects 30 examples can be trusted.\n",
    "\n",
    "Validation set size > 30000 Examples.  If data set is small. Consider cross validation.\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "Gradient descent does not scale.  it uses all the data to compute theloss function.  We want to be able to train on big data.  Instead we use the Sochastic Gradient descent.  We compute the average loss for a very small fraction of the training data.  Has to be random!! This gives a bad estimate, so we have to interate many more times.  At times it may take us in the wrong direction.  We take little steps.  Smaller learning rate.  Scales well with both data and model size. Comes with a lot of issues in practice because of its bed estimates.\n",
    "\n",
    "# Momentum and Learning Rate Decay\n",
    "![momentum](Lesson6/momentum.png)\n",
    "![learning rate](Lesson6/learning-rate.png)\n",
    "\n",
    "# Parameter Hyperspace!\n",
    "Never trust how quickly you learn.  It is usually a bad indicator of how well you learn.  A larger learning rate does not mean that you will learn faster!!\n",
    "\n",
    "Many Hyper-Parameters:\n",
    "* **Initial Learning Rate**\n",
    "* **Learning Rate Decay**\n",
    "* **Momentum**\n",
    "* **Batch Size**\n",
    "* **Weight Initialization**\n",
    "\n",
    "Lots of good solutions for small models, by non satisfactory for large models.\n",
    "\n",
    "Check out ADAGRAD\n",
    "\n",
    "# Mini-batching\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Useful when combined with SGD.  Randomly shuffle the data at the start of each epoch, then create mini-batches.  For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "```\n",
    "The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory. You could train this whole dataset on most CPUs and GPUs.\n",
    "\n",
    "Use mini-batching to run larger models on machines.\n",
    "\n",
    "## TensorFlow Mini-batching\n",
    "### quiz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-56a588522988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data.\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```\n",
    "\n",
    "The accuracy only reached 0.86, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
